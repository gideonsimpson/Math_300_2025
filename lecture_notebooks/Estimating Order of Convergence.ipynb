{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8df6e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee466940",
   "metadata": {},
   "source": [
    "Many of our numerical differentiation and integration methods take the form\n",
    "$$\n",
    "\\text{Absolute Error at $h$} = E(h) =  C h^p + \\mathrm{O}(h^{p+1}),\n",
    "$$\n",
    "so that to leading order $E(h) \\approx C h^p$.  $p$ is the **order** of the method.  We can estimate this from data by noting:\n",
    "$$\n",
    "\\ln E(h) \\approx \\ln C + p \\ln (h),\n",
    "$$\n",
    "so that on $\\log-\\log$ plot, $p$ will be the slope of the line.  This $p$ can then be estimated by linear regression methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffbe07e",
   "metadata": {},
   "source": [
    "# Example\n",
    "Recall that if $f(x) = \\arctan(x)$, $f'(x) = 1/(1+x^2)$.  Using the centered midpoint formula, we will estimate $f'(1)=0.5$ at a series of values of $h$ and try to recover the theoretical $p=2$ order of convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b7c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = x->atan(x);\n",
    "x0 = 1;\n",
    "h_vals = 0.1 * 2. .^(0:-1:-4);\n",
    "df_vals = [];\n",
    "err_vals = [];\n",
    "for h in h_vals\n",
    "    df = (f(x0+h)-f(x0-h))/(2*h); # 3 point centered difference scheme\n",
    "    push!(df_vals, df);\n",
    "    err = abs(df -0.5); # compute absolute error\n",
    "    push!(err_vals, err);\n",
    "    println(\"h = $(round(h, digits=5)), df = $df, Abs. Err. = $err\")\n",
    "end\n",
    "scatter(h_vals, err_vals, xscale=:log10, yscale=:log10, label=\"Data\", xlabel=\"h\", ylabel=\"Abs. Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da236eb",
   "metadata": {},
   "source": [
    "Let us compare, empirically with several values of $p$.   We empircally put the values 0.01 and 0.1 to get the curves to be visually helpful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5cf0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "scatter(h_vals, err_vals, xscale=:log10, yscale=:log10, label=\"Data\", xlabel=\"h\", ylabel=\"Abs. Error\", legend=:bottomright)\n",
    "plot!(h_vals, 0.01 * h_vals, lw=2, label=\"h\", line=:dash)\n",
    "plot!(h_vals, 0.1 * h_vals.^2, lw=2, label=\"h^2\", line=:dash)\n",
    "plot!(h_vals, h_vals.^3, lw=2, label=\"h^3\", line=:dash)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1999397",
   "metadata": {},
   "source": [
    "It lines up well with $\\propto h^2$, which is what we know about the method from previous lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b26605b",
   "metadata": {},
   "source": [
    "We can be a bit more systematic by performing a linear regression.  This requires packaging our data into a `DataFrame` and then using `GLM` to perform the regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b284a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames, GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5367be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataFrame(y = log.(err_vals), x = log.(h_vals))\n",
    "model = lm(@formula(y ~ x), data) # fits a linear model of y = c0 + c1 x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab1ca0d",
   "metadata": {},
   "source": [
    "This finds a slope of $1.9903\\approx 2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9a8eed",
   "metadata": {},
   "source": [
    "# Estimating the order of convergence without the true value\n",
    "Now, we will use our highest precision result, stored in `df_vals[end]`, as a surrogate for the true value, and estimate the order with that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f26c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "est_err = abs.(df_vals[1:end-1] .- df_vals[end]); #estimate the error using highest precision result as a surrogate for \"exact\"\n",
    "\n",
    "scatter(h_vals[1:end-1], est_err, xscale=:log10, yscale=:log10, label=\"Data\", xlabel=\"h\", ylabel=\"Est. Error\")\n",
    "plot!(h_vals, 0.01 * h_vals, lw=2, label=\"h\", line=:dash)\n",
    "plot!(h_vals, 0.1 * h_vals.^2, lw=2, label=\"h^2\", line=:dash)\n",
    "plot!(h_vals, h_vals.^3, lw=2, label=\"h^3\", line=:dash)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c760fef8",
   "metadata": {},
   "source": [
    "Again, it looks quadratic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c25a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataFrame(y = log.(est_err), x = log.(h_vals[1:end-1]))\n",
    "model = lm(@formula(y ~ x), data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a185024d",
   "metadata": {},
   "source": [
    "This (`2.12849`) is not quite as good , but it certainly suggestive of quadratic convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.1",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
